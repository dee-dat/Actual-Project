{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1/142\n",
      "Processing image 2/142\n",
      "Processing image 3/142\n",
      "Processing image 4/142\n",
      "Processing image 5/142\n",
      "Processing image 6/142\n",
      "Processing image 7/142\n",
      "Processing image 8/142\n",
      "Processing image 9/142\n",
      "Processing image 10/142\n",
      "Processing image 11/142\n",
      "Processing image 12/142\n",
      "Processing image 13/142\n",
      "Processing image 14/142\n",
      "Processing image 15/142\n",
      "Processing image 16/142\n",
      "Processing image 17/142\n",
      "Processing image 18/142\n",
      "Processing image 19/142\n",
      "Processing image 20/142\n",
      "Processing image 21/142\n",
      "Processing image 22/142\n",
      "Processing image 23/142\n",
      "Processing image 24/142\n",
      "Processing image 25/142\n",
      "Processing image 26/142\n",
      "Processing image 27/142\n",
      "Processing image 28/142\n",
      "Processing image 29/142\n",
      "Processing image 30/142\n",
      "Processing image 31/142\n",
      "Processing image 32/142\n",
      "Processing image 33/142\n",
      "Processing image 34/142\n",
      "Processing image 35/142\n",
      "Processing image 36/142\n",
      "Processing image 37/142\n",
      "Processing image 38/142\n",
      "Processing image 39/142\n",
      "Processing image 40/142\n",
      "Processing image 41/142\n",
      "Processing image 42/142\n",
      "Processing image 43/142\n",
      "Processing image 44/142\n",
      "Processing image 45/142\n",
      "Processing image 46/142\n",
      "Processing image 47/142\n",
      "Processing image 48/142\n",
      "Processing image 49/142\n",
      "Processing image 50/142\n",
      "Processing image 51/142\n",
      "Processing image 52/142\n",
      "Processing image 53/142\n",
      "Processing image 54/142\n",
      "Processing image 55/142\n",
      "Processing image 56/142\n",
      "Processing image 57/142\n",
      "Processing image 58/142\n",
      "Processing image 59/142\n",
      "Processing image 60/142\n",
      "Processing image 61/142\n",
      "Processing image 62/142\n",
      "Processing image 63/142\n",
      "Processing image 64/142\n",
      "Processing image 65/142\n",
      "Processing image 66/142\n",
      "Processing image 67/142\n",
      "Processing image 68/142\n",
      "Processing image 69/142\n",
      "Processing image 70/142\n",
      "Processing image 71/142\n",
      "Processing image 72/142\n",
      "Processing image 73/142\n",
      "Processing image 74/142\n",
      "Processing image 75/142\n",
      "Processing image 76/142\n",
      "Processing image 77/142\n",
      "Processing image 78/142\n",
      "Processing image 79/142\n",
      "Processing image 80/142\n",
      "Processing image 81/142\n",
      "Processing image 82/142\n",
      "Processing image 83/142\n",
      "Processing image 84/142\n",
      "Processing image 85/142\n",
      "Processing image 86/142\n",
      "Processing image 87/142\n",
      "Processing image 88/142\n",
      "Processing image 89/142\n",
      "Processing image 90/142\n",
      "Processing image 91/142\n",
      "Processing image 92/142\n",
      "Processing image 93/142\n",
      "Processing image 94/142\n",
      "Processing image 95/142\n",
      "Processing image 96/142\n",
      "Processing image 97/142\n",
      "Processing image 98/142\n",
      "Processing image 99/142\n",
      "Processing image 100/142\n",
      "Processing image 101/142\n",
      "Processing image 102/142\n",
      "Processing image 103/142\n",
      "Processing image 104/142\n",
      "Processing image 105/142\n",
      "Processing image 106/142\n",
      "Processing image 107/142\n",
      "Processing image 108/142\n",
      "Processing image 109/142\n",
      "Processing image 110/142\n",
      "Processing image 111/142\n",
      "Processing image 112/142\n",
      "Processing image 113/142\n",
      "Processing image 114/142\n",
      "Processing image 115/142\n",
      "Processing image 116/142\n",
      "Processing image 117/142\n",
      "Processing image 118/142\n",
      "Processing image 119/142\n",
      "Processing image 120/142\n",
      "Processing image 121/142\n",
      "Processing image 122/142\n",
      "Processing image 123/142\n",
      "Processing image 124/142\n",
      "Processing image 125/142\n",
      "Processing image 126/142\n",
      "Processing image 127/142\n",
      "Processing image 128/142\n",
      "Processing image 129/142\n",
      "Processing image 130/142\n",
      "Processing image 131/142\n",
      "Processing image 132/142\n",
      "Processing image 133/142\n",
      "Processing image 134/142\n",
      "Processing image 135/142\n",
      "Processing image 136/142\n",
      "Processing image 137/142\n",
      "Processing image 138/142\n",
      "Processing image 139/142\n",
      "Processing image 140/142\n",
      "Processing image 141/142\n",
      "Processing image 142/142\n",
      "Embedding:99 \n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "import numpy as np\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "dataset = \"dataset\"\n",
    "\n",
    "embeddingFile = \"output/embeddings.pickle\" #initial name for embedding file\n",
    "embeddingModel = \"openface_nn4.small2.v1.t7\" #initializing model for embedding Pytorch\n",
    "\n",
    "#initialization of caffe model for face detection\n",
    "prototxt = \"model/deploy.prototxt\"\n",
    "model =  \"model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "#loading caffe model for face detection\n",
    "#detecting face from Image via Caffe deep learning\n",
    "detector = cv2.dnn.readNetFromCaffe(prototxt, model)\n",
    "\n",
    "#loading pytorch model file for extract facial embeddings\n",
    "#extracting facial embeddings via deep learning feature extraction\n",
    "embedder = cv2.dnn.readNetFromTorch(embeddingModel)\n",
    "\n",
    "#gettiing image paths\n",
    "imagePaths = list(paths.list_images(dataset))\n",
    "\n",
    "#initialization\n",
    "knownEmbeddings = []\n",
    "knownNames = []\n",
    "total = 0\n",
    "conf = 0.5\n",
    "\n",
    "#we start to read images one by one to apply face detection and embedding\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    print(\"Processing image {}/{}\".format(i + 1,len(imagePaths)))\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=600)\n",
    "    (h, w) = image.shape[:2]\n",
    "    #converting image to blob for dnn face detection\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    #setting input blob image\n",
    "    detector.setInput(imageBlob)\n",
    "    #prediction the face\n",
    "    detections = detector.forward()\n",
    "\n",
    "    if len(detections) > 0:\n",
    "        i = np.argmax(detections[0, 0, :, 2])\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        if confidence > conf:\n",
    "            #ROI range of interest\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "            #image to blob for face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "            #facial features embedder input image face blob\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    "            knownNames.append(name)\n",
    "            knownEmbeddings.append(vec.flatten())\n",
    "            total += 1\n",
    "\n",
    "print(\"Embedding:{0} \".format(total))\n",
    "data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "f = open(embeddingFile, \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n",
    "print(\"Process Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
