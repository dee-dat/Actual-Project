{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1/147\n",
      "Processing image 2/147\n",
      "Processing image 3/147\n",
      "Processing image 4/147\n",
      "Processing image 5/147\n",
      "Processing image 6/147\n",
      "Processing image 7/147\n",
      "Processing image 8/147\n",
      "Processing image 9/147\n",
      "Processing image 10/147\n",
      "Processing image 11/147\n",
      "Processing image 12/147\n",
      "Processing image 13/147\n",
      "Processing image 14/147\n",
      "Processing image 15/147\n",
      "Processing image 16/147\n",
      "Processing image 17/147\n",
      "Processing image 18/147\n",
      "Processing image 19/147\n",
      "Processing image 20/147\n",
      "Processing image 21/147\n",
      "Processing image 22/147\n",
      "Processing image 23/147\n",
      "Processing image 24/147\n",
      "Processing image 25/147\n",
      "Processing image 26/147\n",
      "Processing image 27/147\n",
      "Processing image 28/147\n",
      "Processing image 29/147\n",
      "Processing image 30/147\n",
      "Processing image 31/147\n",
      "Processing image 32/147\n",
      "Processing image 33/147\n",
      "Processing image 34/147\n",
      "Processing image 35/147\n",
      "Processing image 36/147\n",
      "Processing image 37/147\n",
      "Processing image 38/147\n",
      "Processing image 39/147\n",
      "Processing image 40/147\n",
      "Processing image 41/147\n",
      "Processing image 42/147\n",
      "Processing image 43/147\n",
      "Processing image 44/147\n",
      "Processing image 45/147\n",
      "Processing image 46/147\n",
      "Processing image 47/147\n",
      "Processing image 48/147\n",
      "Processing image 49/147\n",
      "Processing image 50/147\n",
      "Processing image 51/147\n",
      "Processing image 52/147\n",
      "Processing image 53/147\n",
      "Processing image 54/147\n",
      "Processing image 55/147\n",
      "Processing image 56/147\n",
      "Processing image 57/147\n",
      "Processing image 58/147\n",
      "Processing image 59/147\n",
      "Processing image 60/147\n",
      "Processing image 61/147\n",
      "Processing image 62/147\n",
      "Processing image 63/147\n",
      "Processing image 64/147\n",
      "Processing image 65/147\n",
      "Processing image 66/147\n",
      "Processing image 67/147\n",
      "Processing image 68/147\n",
      "Processing image 69/147\n",
      "Processing image 70/147\n",
      "Processing image 71/147\n",
      "Processing image 72/147\n",
      "Processing image 73/147\n",
      "Processing image 74/147\n",
      "Processing image 75/147\n",
      "Processing image 76/147\n",
      "Processing image 77/147\n",
      "Processing image 78/147\n",
      "Processing image 79/147\n",
      "Processing image 80/147\n",
      "Processing image 81/147\n",
      "Processing image 82/147\n",
      "Processing image 83/147\n",
      "Processing image 84/147\n",
      "Processing image 85/147\n",
      "Processing image 86/147\n",
      "Processing image 87/147\n",
      "Processing image 88/147\n",
      "Processing image 89/147\n",
      "Processing image 90/147\n",
      "Processing image 91/147\n",
      "Processing image 92/147\n",
      "Processing image 93/147\n",
      "Processing image 94/147\n",
      "Processing image 95/147\n",
      "Processing image 96/147\n",
      "Processing image 97/147\n",
      "Processing image 98/147\n",
      "Processing image 99/147\n",
      "Processing image 100/147\n",
      "Processing image 101/147\n",
      "Processing image 102/147\n",
      "Processing image 103/147\n",
      "Processing image 104/147\n",
      "Processing image 105/147\n",
      "Processing image 106/147\n",
      "Processing image 107/147\n",
      "Processing image 108/147\n",
      "Processing image 109/147\n",
      "Processing image 110/147\n",
      "Processing image 111/147\n",
      "Processing image 112/147\n",
      "Processing image 113/147\n",
      "Processing image 114/147\n",
      "Processing image 115/147\n",
      "Processing image 116/147\n",
      "Processing image 117/147\n",
      "Processing image 118/147\n",
      "Processing image 119/147\n",
      "Processing image 120/147\n",
      "Processing image 121/147\n",
      "Processing image 122/147\n",
      "Processing image 123/147\n",
      "Processing image 124/147\n",
      "Processing image 125/147\n",
      "Processing image 126/147\n",
      "Processing image 127/147\n",
      "Processing image 128/147\n",
      "Processing image 129/147\n",
      "Processing image 130/147\n",
      "Processing image 131/147\n",
      "Processing image 132/147\n",
      "Processing image 133/147\n",
      "Processing image 134/147\n",
      "Processing image 135/147\n",
      "Processing image 136/147\n",
      "Processing image 137/147\n",
      "Processing image 138/147\n",
      "Processing image 139/147\n",
      "Processing image 140/147\n",
      "Processing image 141/147\n",
      "Processing image 142/147\n",
      "Processing image 143/147\n",
      "Processing image 144/147\n",
      "Processing image 145/147\n",
      "Processing image 146/147\n",
      "Processing image 147/147\n",
      "Embedding:147 \n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "import numpy as np\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "dataset = \"dataset\"\n",
    "\n",
    "embeddingFile = \"output/embeddings.pickle\" #initial name for embedding file\n",
    "embeddingModel = \"openface_nn4.small2.v1.t7\" #initializing model for embedding Pytorch\n",
    "\n",
    "#initialization of caffe model for face detection\n",
    "prototxt = \"model/deploy.prototxt\"\n",
    "model =  \"model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "#loading caffe model for face detection\n",
    "#detecting face from Image via Caffe deep learning\n",
    "detector = cv2.dnn.readNetFromCaffe(prototxt, model)\n",
    "\n",
    "#loading pytorch model file for extract facial embeddings\n",
    "#extracting facial embeddings via deep learning feature extraction\n",
    "embedder = cv2.dnn.readNetFromTorch(embeddingModel)\n",
    "\n",
    "#gettiing image paths\n",
    "imagePaths = list(paths.list_images(dataset))\n",
    "\n",
    "#initialization\n",
    "knownEmbeddings = []\n",
    "knownNames = []\n",
    "total = 0\n",
    "conf = 0.5\n",
    "\n",
    "#we start to read images one by one to apply face detection and embedding\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    print(\"Processing image {}/{}\".format(i + 1,len(imagePaths)))\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=600)\n",
    "    (h, w) = image.shape[:2]\n",
    "    #converting image to blob for dnn face detection\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    #setting input blob image\n",
    "    detector.setInput(imageBlob)\n",
    "    #prediction the face\n",
    "    detections = detector.forward()\n",
    "\n",
    "    if len(detections) > 0:\n",
    "        i = np.argmax(detections[0, 0, :, 2])\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        if confidence > conf:\n",
    "            #ROI range of interest\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "            #image to blob for face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "            #facial features embedder input image face blob\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    "            knownNames.append(name)\n",
    "            knownEmbeddings.append(vec.flatten())\n",
    "            total += 1\n",
    "\n",
    "print(\"Embedding:{0} \".format(total))\n",
    "data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "f = open(embeddingFile, \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n",
    "print(\"Process Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
